---
title: "All Things Regression"
subtitle: "Part II"
author: "Fernando Hoces la Guardia"
date: "07/21/2022"
output: 
  xaringan::moon_reader:
    footer: "These slides available at https://fhoces.github.io/econ140summer2022/"
    css: [default, metropolis, metropolis-fonts, "my-css.css"] 
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

<style type="text/css">
.remark-slide-content {
    font-size: 30px;
    padding: 1em 1em 1em 1em;
}
</style>

```{r , include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(ggthemes, viridis, knitr, extrafont, tidyverse, magrittr, latex2exp, 
       parallel, Ecdat, wooldridge, dslabs, ggforce, emo, png, grid, pander, 
       countdown, emoGG, haven, broom, ggridges, gridExtra, kableExtra, snakecase, 
       janitor, data.table, lubridate, lfe, here, gapminder, AER, stargazer, margins, 
       furrr)


# Define pink color
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#3b3b9a"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
slate <- "#314f4f"
# Dark slate grey: #314f4f

options(htmltools.dir.version = FALSE)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])

overwrite = FALSE


# Define colors
red_pink <- "#e64173"
met_slate <- "#23373b" # metropolis font color
# Notes directory
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  #dpi = 300,
  #cache = T,
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE
  #fig.width = 11,
  #fig.height = 5
)

theme_simple <- theme_bw() + theme(
  axis.line = element_line(color = met_slate),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  text = element_text(family = "Fira Sans", color = met_slate, size = 14),
  axis.text.x = element_text(size = 12),
  axis.text.y = element_text(size = 12),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  legend.position = "none"
)
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -0.5, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 18, family = "STIXGeneral"),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_math <- theme_void() + theme(
  text = element_text(family = "MathJax_Math"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_serif <- theme_void() + theme(
  text = element_text(family = "MathJax_Main"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes <- theme_void() + theme(
  text = element_text(family = "Fira Sans Book"),
  axis.title = element_text(size = 18),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = grey_light,
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_set(theme_gray(base_size = 20))
# Column names for regression results
reg_columns <- c("Term", "Est.", "S.E.", "t stat.", "p-Value")
# Function for formatting p values
format_pvi <- function(pv) {
  return(ifelse(
    pv < 0.0001,
    "<0.0001",
    round(pv, 4) %>% format(scientific = F)
  ))
}
format_pv <- function(pvs) lapply(X = pvs, FUN = format_pvi) %>% unlist()
# Tidy regression results table
tidy_table <- function(x, terms, highlight_row = 1, highlight_color = "black", highlight_bold = T, digits = c(NA, 3, 3, 2, 5), title = NULL) {
  x %>%
    tidy() %>%
    select(1:5) %>%
    mutate(
      term = terms,
      p.value = p.value %>% format_pv()
    ) %>%
    kable(
      col.names = reg_columns,
      escape = F,
      digits = digits,
      caption = title
    ) %>%
    kable_styling(font_size = 20) %>%
    row_spec(1:nrow(tidy(x)), background = "white") %>%
    row_spec(highlight_row, bold = highlight_bold, color = highlight_color)
}


```



```{css, echo = F, eval = T}
@media print {
  .has-continuation {
    display: block !important;
  }
}
```


# Housekeeping 

- Let's choose the chapter for the summary (still due Friday 5pm on gradescope) 

- Practice questions are up. Midterm will follow similar questions (but not exactly the same ones). 
  - Goal: if you understood the concepts behind the practice questions, you will do well in the midterm. 
  
- Switching the order of the review session: will do a review on Monday (before the midterm), and on Wednesday we will start with new material. Bring questions! (I will not bring new material, if we finish early we can watch the first part of Run Lola Run)

---

# Regression Journey

- Regression as Matching on Groups. Ch2 of MM up to page 68 (not included).

- Regression as Line Fitting and Conditional Expectation. Ch2 of MM, Appendix.

- Multiple Regression and Omitted Variable Bias. Ch2 of MM pages 68-79 and Appendix. 
- All Things Regression: Anatomy, Inference, Logarithms, Binary Outcomes, and $R^2$. Ch2 of MM, Appendix + others. 

---
# Regression Journey

- Regression as Matching on Groups. Ch2 of MM up to page 68 (not included).

- Regression as Line Fitting and Conditional Expectation. Ch2 of MM, Appendix.

- Multiple Regression and Omitted Variable Bias. Ch2 of MM pages 68-79 and Appendix.

- **All Things Regression: Anatomy, Inference, Logarithms, Binary Outcomes, and $R^2$. Ch2 of MM, Appendix + others.** 


---

# Today and Tomorrow's Lecture

- Regression Anatomy 

- Regression Inference

- ** $R^2$ **

- Non-linearities:
   - Logarithms
   - Others 

- Binary Outcomes



---

# Analysis of Variance

* Remember that $Y_i = \widehat{Y_i} + e_i$.

* We have the following decomposition

$$
\begin{aligned}
Var(Y) &= Var( \widehat Y + e) \\
&= Var(\widehat Y ) + Var(e) + 2 Cov(\widehat Y,e) \\
&= Var(\widehat Y ) + Var(e)
\end{aligned}
$$



    
* __Total variation (SST) = Model explained (SSE) + Unexplained (SSR)__

* Because:
  * $Var(x+y) = Var(x) + Var(y) + 2Cov(x,y)$
  * $Cov(\hat{Y},e)=0$

---

# Goodness of Fit

.font90[
* The __ $R^2$ __ measures how well the __model fits the data__.
]
--

.font90[


$$
\begin{equation}
R^2 = \frac{\text{variance explained}}{\text{total variance}} =     \frac{SSE}{SST} = 1 - \frac{SSR}{SST}\in[0,1]
\end{equation}
$$
]

--
.font90[    
* $R^2$ close to $1$ indicates a __very ***high*** explanatory power__ of the model.

* $R^2$ close to $0$ indicates a __very ***low*** explanatory power__ of the model.
]

--
.font90[
* *Interpretation:* an $R^2$ of 0.5, for example, means that the variation in $x$ "explains" 50% of the variation in $y$.
]
--
.font90[    
* `r emo::ji("warning")` Low $R^2$ does __NOT__ mean it's a useless model! Remember that econometrics is interested in causal mechanisms, not prediction!
]
--
.font90[    
* `r emo::ji("warning")` The $R^2$ is __NOT__ an indicator of whether a relationship is causal!
]


---
class: inverse, middle

# Non-linearities

---
# Non-linearities

```{R, include = F}
reg_lin <- lm(lifeExp ~ gdpPercap, gapminder)
a_lin <- reg_lin$coefficients[1]
b_lin <- reg_lin$coefficients[2]
r2_lin <- summary(reg_lin)$r.squared
```


$$(\widehat{\text{Life Expectancy})_i} = `r round(a_lin, 2)` + `r round(b_lin, 4)` \cdot \text{GDP}_i$$

```{R, echo = F, dev = "svg", fig.height = 5}
ggplot(data = gapminder, aes(x = gdpPercap, y = lifeExp)) +
geom_point(alpha = 0.75) +
scale_x_continuous("GDP per capita", label = scales::comma) +
ylab("Life Expectancy") +
stat_smooth(method = "lm", size = 1, color = red_pink, se = F) +
theme_pander(base_size = 17, base_family = "Fira Sans", fc = met_slate)
```

---
# Nonlinear Relationships

Erroneus critique of regression: "many economic relationships are **nonlinear** (*e.g.*, most production functions, profit, diminishing marginal utility, tax revenue as a function of the tax rate, *etc.*), hence fitting straight lines is a bad way of estimating such relationships"

--

**The flexibility of regression**
OLS estimation can accommodate many, but not all, nonlinear relationships.

- Underlying model must be linear-in-parameters.

- Nonlinear transformations of variables are okay.

- We have already seen the most nonlinear set of variables that you can imagine (binaries!) 

---
# Linearity

.hi-green[Linear-in-parameters:] .green[Parameters] enter model as a weighted sum, where the weights are functions of the variables.

- This is the one required to estimate OLS

.hi-pink[Linear-in-variables:] .pink[Variables] enter the model as a weighted sum, where the weights are functions of the parameters.

- This is the one the critique was pointing at. OLS works perfectly here. 

--

The standard linear regression model satisfies both properties:

$$Y_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \dots + \beta_kX_{ki} + e_i$$

---
# Linearity

Which of the following is .hi-green[linear-in-parameters], .hi-pink[linear-in-variables], or .hi-purple[neither]?

1. $Y_i = \beta_0 + \beta_1X_{i} + \beta_2X_{i}^2 + \dots + \beta_kX_{i}^k + e_i$

2. $Y_i = \beta_0X_i^{\beta_1}e_i$

3. $Y_i = \beta_0 + \beta_1\beta_2X_{i} + e_i$

---
count: false

# Linearity

Which of the following is .hi-green[linear-in-parameters], .hi-pink[linear-in-variables], or .hi-purple[neither]?

1. $\color{#007935}{Y_i = \beta_0 + \beta_1X_{i} + \beta_2X_{i}^2 + \dots + \beta_kX_{i}^k + e_i}$

2. $Y_i = \beta_0X_i^{\beta_1}e_i$

3. $Y_i = \beta_0 + \beta_1\beta_2X_{i} + e_i$

Model 1 is .green[linear-in-parameters], but not linear-in-variables. 

---
count: false

# Linearity

Which of the following is .hi-green[linear-in-parameters], .hi-pink[linear-in-variables], or .hi-purple[neither]?

1. $\color{#007935}{Y_i = \beta_0 + \beta_1X_{i} + \beta_2X_{i}^2 + \dots + \beta_kX_{i}^k + e_i}$

2. $\color{#9370DB}{Y_i = \beta_0X_i^{\beta_1}e_i}$

3. $Y_i = \beta_0 + \beta_1\beta_2X_{i} + e_i$

Model 1 is .green[linear-in-parameters], but not linear-in-variables. 

Model 2 is .purple[neither]. 

---
count: false

# Linearity

Which of the following is .hi-green[linear-in-parameters], .hi-pink[linear-in-variables], or .hi-purple[neither]?

1. $\color{#007935}{Y_i = \beta_0 + \beta_1X_{i} + \beta_2X_{i}^2 + \dots + \beta_kX_{i}^k + e_i}$

2. $\color{#9370DB}{Y_i = \beta_0X_i^{\beta_1}e_i}$

3. $\color{#e64173}{Y_i = \beta_0 + \beta_1\beta_2X_{i} + e_i}$

Model 1 is .green[linear-in-parameters], but not linear-in-variables. 

Model 2 is .purple[neither]. 

Model 3 is .pink[linear-in-variables], but not linear-in-parameters.

---
# We're Going to Take Logs

The natural log is the inverse function for the exponential function: <br> $\quad \log(e^x) = x$ for $x>0$.

## (Natural) Log Rules and Approximations

1. Product rule: $\log(AB) = \log(A) + \log(B)$.

2. Quotient rule: $\log(A/B) = \log(A) - \log(B)$.

3. Power rule: $\log(A^B) = B \cdot \log(A)$.

4. $\log(e) = 1$, $\log(1) = 0$, and $\log(x)$ is undefined for $x \leq 0$.

5. **Approximation: $\log(1 + A) = A$ If A is very small (~less than 0.2)**
---
# Log-Linear Model
.font80[
**Nonlinear Model** $$Y_i = \alpha e^{\beta_1 X_i}e_i$$

- $Y > 0$, $X$ is continuous, and $e_i$ is a multiplicative error term.
- Cannot estimate parameters with OLS directly.

**Logarithmic Transformation** $$\log(Y_i) = \log(\alpha) + \beta_1 X_i + \log(e_i)$$

- Redefine $\log(\alpha) \equiv \beta_0$ and $\log(e_i) \equiv e_i$.

**Transformed (Linear) Model** $$\log(Y_i) = \beta_0 + \beta_1 X_i + e_i$$

- *Can* estimate with OLS, but coefficient interpretation changes.
]
---
# Log-Linear Model

$$\log(Y_i) = \beta_0 + \beta_1 X_i + e_i$$

**Interpretation**

- A one-unit increase in the explanatory variable increases the outcome variable by approximately $\beta_1\times 100$ percent, on average.

- *Example:* If $\log(\hat{\text{Pay}_i}) = 2.9 + 0.03 \cdot \text{School}_i$, then an additional year of schooling increases pay by approximately 3 percent, on average.

---
count:false
# Why?


```{R, include = F}
# Set seed
set.seed(1234)
# Sample size
n <- 1e3
# Generate data
df1 <- tibble(
  x = 1:100/100, 
  y = log(1 + x)
)

```

.font80[

.pull-left[

- We want to know how to interpret what is the associated increase in $Y$, when we increase $X$ in one unit.


$$
\begin{aligned}
\log(Y_i) &= \beta_0 + \beta_1 X_i + e_i\\
\widetilde \log(Y_i) &= \beta_0 + \beta_1 (X_i+1) + e_i
\end{aligned}
$$

]
]

---
count:false
# Why?


```{R, include = F}
# Set seed
set.seed(1234)
# Sample size
n <- 1e3
# Generate data
df1 <- tibble(
  x = 1:100/100, 
  y = log(1 + x)
)

```

.font80[

.pull-left[

- We want to know how to interpret what is the associated increase in $Y$, when we increase $X$ in one unit.

$$
\begin{aligned}
\log(Y_i) &= \beta_0 + \beta_1 X_i + e_i\\
\widetilde \log(Y_i) &= \beta_0 + \beta_1 (X_i+1) + e_i\\
\widetilde \log(Y_i) - \log(Y_i) &= \beta_0 + \beta_1 X_i+ \beta_1 + e_i - \\
& \quad (\beta_0 + \beta_1 X_i + e_i)\\
\widetilde \log(Y_i) - \log(Y_i) &= \beta_1
\end{aligned}
$$

]



.pull-right[
.center[**Comparing log(1+X) with X**]
```{R, echo = F, cache = T, dev = "svg", fig.height = 5}
# Plot
ggplot(data = df1, aes(x = x, y = y)) +
geom_point(size = 3, color = "darkslategrey", alpha = 0.5) +
geom_line(aes(x = x, y = x))+  
xlab("X") +
ylab("log(1+X)") +
theme_pander(base_size = 17, fc = met_slate)
```


]
]

---
# Why?


```{R, include = F}
# Set seed
set.seed(1234)
# Sample size
n <- 1e3
# Generate data
df1 <- tibble(
  x = 1:100/100, 
  y = log(1 + x)
)

```

.font80[

.pull-left[

- We want to know how to interpret what is the associated increase in $Y$, when we increase $X$ in one unit.

$$
\begin{aligned}
\log(Y_i) &= \beta_0 + \beta_1 X_i + e_i\\
\widetilde \log(Y_i) &= \beta_0 + \beta_1 (X_i+1) + e_i\\
\widetilde \log(Y_i) - \log(Y_i) &= \beta_0 + \beta_1 X_i+ \beta_1 + e_i - \\
& \quad (\beta_0 + \beta_1 X_i + e_i)\\
\widetilde \log(Y_i) - \log(Y_i) &= \beta_1\\
\widetilde \log(Y_i)  &= \log(Y_i) + \beta_1\\
\widetilde \log(Y_i)  &\approx \log(Y_i)  + log(1+\beta_1) \\
\widetilde \log(Y_i)  &\approx \log(Y_i(1+\beta_1))\\
\widetilde Y_i  &\approx Y_i(1+\beta_1)
\end{aligned}
$$

]



.pull-right[
.center[**Comparing log(1+X) with X**]
```{R, echo = F, cache = T, dev = "svg", fig.height = 5}
# Plot
ggplot(data = df1, aes(x = x, y = y)) +
geom_point(size = 3, color = "darkslategrey", alpha = 0.5) +
geom_line(aes(x = x, y = x))+  
#geom_point(aes(x = x, y = exp(x)-1 ))+  
xlab("X") +
ylab("log(1+X)") +
theme_pander(base_size = 17, fc = met_slate)
```

- A one-unit increase in the explanatory variable increases the outcome variable by approximately $\beta_1\times 100$ percent, on average.
- What if $\beta_1$ is large (>0.2)? No problem, just divide X by 10, 100, or larger, to shrink the units of $\beta_1$. 

]
]


---
# (If X is Binary and $\beta > 0.2$: Use Exact)


.font80[

$$
\begin{aligned}
\log(Y_i) &= \beta_0 + \beta_1 X_i + e_i\\
\widetilde \log(Y_i) &= \beta_0 + \beta_1 (X_i+1) + e_i\\
\widetilde \log(Y_i) - \log(Y_i) &= \beta_0 + \beta_1 X_i+ \beta_1 + e_i - \\
& \quad (\beta_0 + \beta_1 X_i + e_i)
\end{aligned}
$$
.pull-left[
.center[Exact]

$$
\begin{aligned}
\widetilde \log(Y_i) - \log(Y_i) &= \beta_1\\
\log(\widetilde Y_i / Y_i) &= \beta_1\\
\widetilde Y_i / Y_i &= e^{\beta_1}\\
(\widetilde Y_i -  Y_i)/ Y_i &= e^{\beta_1} - 1 \text{ From } X = 0 \text{ to }X = 1\\
(\widetilde Y_i -  Y_i)/ Y_i &= e^{-\beta_1} - 1 \text{ From } X = 1 \text{ to }X = 0
\end{aligned}
$$

]



.pull-right[

.center[Approximation]
$$
\begin{aligned}
\widetilde \log(Y_i)  &= \log(Y_i) + \beta_1\\
\widetilde \log(Y_i)  &\approx \log(Y_i)  + log(1+\beta_1) \\
\widetilde \log(Y_i)  &\approx \log(Y_i(1+\beta_1))\\
\widetilde Y_i  &\approx Y_i(1+\beta_1)
\end{aligned}
$$
]

- If we cannot re-scale $X$ to have a small $\beta$ we need to compute the percentage difference using the exact formula (left). Also, interpretation from 1 to 0 does not work well in approximation. 

]


---
# Log-Linear Example

```{R, include = F}
# Set seed
set.seed(1234)
# Sample size
n <- 1e3
# Generate data
df1 <- tibble(
  x = runif(n, 0, 3),
  y = exp(10 + 0.75 * x + rnorm(n, sd = 0.5))
)
reg1 <- lm(log(y) ~ x, df1)
a1 <- reg1$coefficients[1]
b1 <- reg1$coefficients[2]
```

$$\log(\hat{Y_i}) = `r round(a1, 2)` + `r round(b1, 2)` \cdot \text{X}_i$$

```{R, log linear plot, echo = F, cache = T, dev = "svg", fig.height = 5}
# Plot
ggplot(data = df1, aes(x = x, y = y)) +
geom_point(size = 3, color = "darkslategrey", alpha = 0.5) +
geom_smooth(color = red_pink, se = F) +
xlab("X") +
ylab("Y") +
theme_pander(base_size = 17, base_family = "Fira Sans", fc = met_slate)
```

---
count: false

# Log-Linear Example

$$\log(\hat{Y_i}) = `r round(a1, 2)` + `r round(b1, 2)` \cdot \text{X}_i$$

```{R, log linear plot 2, echo = F, cache = T, dev = "svg", fig.height = 5}
# Plot
ggplot(data = df1, aes(x = x, y = log(y))) +
geom_point(size = 3, color = "darkslategrey", alpha = 0.5) +
geom_smooth(color = red_pink, se = F, method = "lm") +
xlab("X") +
ylab("log(Y)") +
theme_pander(base_size = 17, base_family = "Fira Sans", fc = met_slate)
```

---
# Log-Log Model

.font90[
**Nonlinear Model**

$$Y_i = \alpha  X_i^{\beta_1}e_i$$

- $Y > 0$, $X > 0$, and $e_i$ is a multiplicative error term.
- Cannot estimate parameters with OLS directly.
]
--
.font90[
**Logarithmic Transformation**

$$\log(Y_i) = \log(\alpha) + \beta_1 \log(X_i) + \log(e_i)$$

- Redefine $\log(\alpha) \equiv \beta_0$ and $\log(e_i) \equiv e_i$. 
]
--
.font90[
**Transformed (Linear) Model**

$$\log(Y_i) = \beta_0 + \beta_1 \log(X_i) + e_i$$

- *Can* estimate with OLS, but coefficient interpretation changes.
]
---
# Log-Log Model

**Regression Model**

$$ \log(Y_i) = \beta_0 + \beta_1 \log(X_i) + e_i $$

**Interpretation**

- A one-percent increase in the explanatory variable leads to a $\beta_1$-percent change in the outcome variable, on average.

- This is the definition of an elasticity in economics $(\Delta\%Q/\Delta\%P)$

- *Example:* If $\log(\widehat{\text{Quantity Demanded}}_i) = 0.45 - 0.31 \cdot \log(\text{Income}_i)$, then each one-percent increase in income decreases quantity demanded by 0.31 percent.

---
# Why?

.font90[
- We want to know how to interpret what is the associated increase in $Y$, when we increase $X$ in **1 percent unit** (differnent from before).

$$
\begin{aligned}
\log(Y_i) = \beta_0 + \beta_1 \log(X_i) + e_i\\
\widetilde \log(Y_i) = \beta_0 + \beta_1 \log(X_i\times 1.01) + e_i\\
\widetilde \log(Y_i) - \log(Y_i) &= \beta_0 + \beta_1 X_i+ \beta_1\log(1.01) + e_i - \\
& \quad (\beta_0 + \beta_1 X_i + e_i)\\
\widetilde \log(Y_i) - \log(Y_i) &= \beta_1 \log(1.01)\\
\widetilde \log(Y_i)  &= \log(Y_i) + \beta_1 \log(1.01)\\
\widetilde \log(Y_i)  &\approx \log(Y_i)  + \beta_1\times 0.01 \\
\widetilde \log(Y_i)  &\approx \log(Y_i)  + \log(1 + \beta_1/100) \\
\widetilde \log(Y_i)  &\approx \log(Y_i(1+\beta_1/100))
\end{aligned}
$$


A one-percent increase in $X$ leads to a $\beta_1$-percent increase in $Y$. 
]
---
# Log-Log Example

```{R, include = F}
# Set seed
set.seed(1234)
# Sample size
n <- 1e3
# Generate data
log_df <- tibble(
  x = runif(n, 0, 10),
  y = exp(3 * log(x) + rnorm(n, sd = 0.5))
)
reg3 <- lm(log(y) ~ log(x), log_df)
a3 <- reg3$coefficients[1]
b3 <- reg3$coefficients[2]
```

$$\log(\hat{Y_i}) = `r round(a3, 2)` + `r round(b3, 2)` \cdot \log(\text{X}_i)$$

```{R, log log plot, echo = F, cache = T, dev = "svg", fig.height = 5}
# Plot
ggplot(data = log_df, aes(x = x, y = y)) +
geom_point(size = 3, color = "darkslategrey", alpha = 0.5) +
geom_smooth(color = red_pink, se = F) +
xlab("X") +
ylab("Y") +
theme_pander(base_size = 17, base_family = "Fira Sans", fc = met_slate)
```

---
count: false

# Log-Log Example

$$\log(\hat{Y_i}) = `r round(a3, 2)` + `r round(b3, 2)` \cdot \log(\text{X}_i)$$

```{R, log log plot 2, echo = F, cache = T, dev = "svg", fig.height = 5}
# Plot
ggplot(data = log_df, aes(x = log(x), y = log(y))) +
geom_point(size = 3, color = "darkslategrey", alpha = 0.5) +
geom_smooth(color = red_pink, se = F, method = "lm") +
xlab("log(X)") +
ylab("log(Y)") +
theme_pander(base_size = 17, base_family = "Fira Sans", fc = met_slate)
```

---
# Linear-Log Model 

.font90[
**Nonlinear Model**

$$e^{Y_i} = \alpha  X_i^{\beta_1}e_i$$

- $X > 0$ and $e_i$ is a multiplicative error term.
- Cannot estimate parameters with OLS directly.
]
--
.font90[
**Logarithmic Transformation**

$$Y_i = \log(\alpha) + \beta_1 \log(X_i) + \log(e_i)$$

- Redefine $\log(\alpha) \equiv \beta_0$ and $\log(e_i) \equiv e_i$.
]
--
.font90[
**Transformed (Linear) Model**

$$Y_i = \beta_0 + \beta_1 \log(X_i) + e_i$$

- *Can* estimate with OLS, but coefficient interpretation changes.
]
---
# Linear-Log Model

**Regression Model**

$$Y_i = \beta_0 + \beta_1 \log(X_i) + e_i$$

**Interpretation**

- A one-percent increase in the explanatory variable increases the outcome variable by approximately $\beta_1 \div 100$, on average.

- *Example:* If $\hat{(\text{Blood Pressure})_i} = 150 - 9.1 \log(\text{Income}_i)$, then a one-percent increase in income decrease blood pressure by 0.091 points.

---
# Linear-Log Example

```{R, include = F}
# Set seed
set.seed(1234)
# Sample size
n <- 1e3
# Generate data
lin_log_df <- tibble(
  x = runif(n, 0, 3),
  y = log(x) + rnorm(n, sd = 0.25)
)
reg4 <- lm(y ~ log(x), lin_log_df)
a4 <- reg4$coefficients[1]
b4 <- reg4$coefficients[2]
```

$$\hat{Y_i} = `r round(a4, 2)` + `r round(b4, 2)` \cdot \log(\text{X}_i)$$

```{R, linear log plot, echo = F, cache = T, dev = "svg", fig.height = 5}
# Plot
ggplot(data = lin_log_df, aes(x = x, y = y)) +
geom_point(size = 3, color = "darkslategrey", alpha = 0.5) +
xlab("X") +
ylab("Y") +
theme_pander(base_size = 17, base_family = "Fira Sans", fc = met_slate)
```

---
count: false

# Linear-Log Example

$$\hat{Y_i} = `r round(a4, 2)` + `r round(b4, 2)` \cdot \log(\text{X}_i)$$

```{R, linear log plot 2, echo = F, cache = T, dev = "svg", fig.height = 5}
# Plot
ggplot(data = lin_log_df, aes(x = log(x), y = y)) +
geom_point(size = 3, color = "darkslategrey", alpha = 0.5) +
geom_smooth(color = red_pink, se = F, method = "lm") +
xlab("log(X)") +
ylab("Y") +
theme_pander(base_size = 17, base_family = "Fira Sans", fc = met_slate)
```

---
#(Approximate) Coefficient Interpretation

.font90[

```{r, echo = F}
cont_interp <- tibble(
  model = c(
    "Level-level <br> 
    \\(Y_i = \\beta_0 + \\beta_1 X_i + e_i\\)",
    "Log-level <br> 
    \\(\\log(Y_i) = \\beta_0 + \\beta_1 X_i + e_i\\)",
    "Log-log <br> 
    \\(\\log(Y_i) = \\beta_0 + \\beta_1 \\log(X_i) + e_i\\)",
    "Level-log <br> 
    \\(Y_i = \\beta_0 + \\beta_1 \\log(X_i) + e_i\\)"
  ),
  interp = c(
    "\\(\\Delta Y = \\beta_1 \\cdot \\Delta X\\) <br> 
    A one-unit increase in \\(X\\) leads to a \\(\\beta_1\\)-unit increase in \\(Y\\)",
    "\\(\\%\\Delta Y = 100 \\cdot \\beta_1 \\cdot \\Delta X\\) <br> 
    A one-unit increase in \\(X\\) leads to a \\(\\beta_1 \\cdot 100\\%\\) increase in \\(Y\\)",
    "\\(\\%\\Delta Y = \\beta_1 \\cdot \\%\\Delta X\\) <br> 
    A one-percent increase in \\(X\\) leads to a \\(\\beta_1\\%\\) increase in \\(Y\\)",
    "\\(\\Delta Y = (\\beta_1 \\div 100) \\cdot \\%\\Delta X\\) <br> 
    A one-percent increase in \\(X\\) leads to a \\(\\beta_1 \\div 100\\)-unit increase in \\(Y\\)"
  )
) %>% 
  kable(
  escape = F,
  col.names = c("Model", "\\(\\beta_1\\) Interpretation"),
  align = c("l", "l")
) %>% 
  column_spec(1, color = "black", bold = T, italic = T, extra_css = "vertical-align:top;") %>% 
  column_spec(2, color = "black", italic = T)

cont_interp
```

]

---
# Can We Do Better?

$$(\widehat{\text{Life Expectancy})_i} = `r round(a_lin, 2)` + `r round(b_lin, 4)` \cdot \text{GDP}_i \quad\quad R^2 = `r round(r2_lin, 2)`$$

```{R, echo = F, dev = "svg", fig.height = 5}
ggplot(data = gapminder, aes(x = gdpPercap, y = lifeExp)) +
geom_point(alpha = 0.75) +
scale_x_continuous("GDP per capita", label = scales::comma) +
ylab("Life Expectancy") +
stat_smooth(method = "lm", size = 1, color = red_pink, se = F) +
theme_pander(base_size = 17, base_family = "Fira Sans", fc = met_slate)
```

---
# Can We Do Better?

```{R, include = F}
reg_log_lin <- lm(log(lifeExp) ~ gdpPercap, gapminder)
a_log_lin <- reg_log_lin$coefficients[1]
b_log_lin <- reg_log_lin$coefficients[2]
r2_log_lin <- summary(reg_log_lin)$r.squared
```

$$\log( \widehat{\text{Life Expectancy}_i}) = `r round(a_log_lin, 2)` + `r round(b_log_lin, 6)` \cdot \text{GDP}_i \quad\quad R^2 = `r round(r2_log_lin, 2)`$$

```{R, echo = F, dev = "svg", fig.height = 5}
ggplot(data = gapminder, aes(x = gdpPercap, y = log(lifeExp))) +
geom_point(alpha = 0.75) +
scale_x_continuous("GDP per capita", label = scales::comma) +
ylab("log(Life Expectancy)") +
stat_smooth(method = "lm", size = 1, color = red_pink, se = F) +
theme_pander(base_size = 17, base_family = "Fira Sans", fc = met_slate)
```

---
# Can We Do Better?

```{R, include = F}
reg_log <- lm(log(lifeExp) ~ log(gdpPercap), gapminder)
a_log <- reg_log$coefficients[1]
b_log <- reg_log$coefficients[2]
r2_log <- summary(reg_log)$r.squared
```

$$\log ( \widehat{\text{Life Expectancy}_i} ) = `r round(a_log, 2)` + `r round(b_log, 2)` \cdot \log \left( \text{GDP}_i \right) \quad\quad R^2 = `r round(r2_log, 2)`$$

```{R, echo = F, dev = "svg", fig.height = 5}
ggplot(data = gapminder, aes(x = log(gdpPercap), y = log(lifeExp))) +
geom_point(alpha = 0.75) +
scale_x_continuous("log(GDP per capita)", label = scales::comma) +
ylab("log(Life Expectancy)") +
stat_smooth(method = "lm", size = 1, color = red_pink, se = F) +
theme_pander(base_size = 17, base_family = "Fira Sans", fc = met_slate)
```

---
# Can We Do Better?

```{R, include = F}
reg_lin_log <- lm(lifeExp ~ log(gdpPercap), gapminder)
a_lin_log <- reg_lin_log$coefficients[1]
b_lin_log <- reg_lin_log$coefficients[2]
r2_lin_log <- summary(reg_lin_log)$r.squared
```

$$( \widehat{\text{Life Expectancy}})_i = `r round(a_lin_log, 2)` + `r round(b_lin_log, 2)` \cdot \log \left( \text{GDP}_i \right) \quad\quad R^2 = `r round(r2_lin_log, 2)`$$

```{R, echo = F, dev = "svg", fig.height = 5}
ggplot(data = gapminder, aes(x = log(gdpPercap), y = lifeExp)) +
geom_point(alpha = 0.75) +
scale_x_continuous("log(GDP per capita)", label = scales::comma) +
ylab("Life Expectancy") +
stat_smooth(method = "lm", size = 1, color = red_pink, se = F) +
theme_pander(base_size = 17, base_family = "Fira Sans", fc = met_slate)
```

---
# Practical Considerations
.font80[
**Consideration 1:** Do your data take negative numbers or zeros as values?

```{r}
log(0)
```

**Consideration 2:** What coefficient interpretation do you want? Unit change? Unit-free percent change?



**Consideration 3:** Are your data skewed?

.pull-left[
```{R, skew 1, echo = F, cache = T, dev = "svg", fig.height = 4}
# Plot
ggplot(data = df1, aes(x = y)) +
geom_histogram(color = red_pink, fill = red_pink, alpha = 0.5) +
xlab("Y") +
ylab("Count") +
theme_pander(base_size = 20, base_family = "Fira Sans", fc = met_slate)
```
]

.pull-right[
```{R, skew 2, echo = F, cache = T, dev = "svg", fig.height = 4}
# Plot
ggplot(data = df1, aes(x = log(y))) +
geom_histogram(color = red_pink, fill = red_pink, alpha = 0.5) +
xlab("log(Y)") +
ylab("Count") +
theme_pander(base_size = 20, base_family = "Fira Sans", fc = met_slate)
```
]
]


---
# .font90[Final Message: Allways Plot Your Data (Anscombe's Quartet)]

.center[**Four *"identical"* regressions:** Intercept .mono[=] 3, Slope .mono[=] 0.5, R.super[2] .mono[=] 0.67]

```{r, echo = F, dev = "svg", fig.height = 5}
anscombe_m <- data.frame()

for(i in 1:4)
  anscombe_m <- rbind(anscombe_m, data.frame(set=i, x=anscombe[,i], y=anscombe[,i+4]))

anscombe_m %>% 
  ggplot(aes(x, y)) + geom_point(size=3, alpha = 0.8, color = met_slate) + 
  geom_smooth(method="lm", fill=NA, fullrange=TRUE, color = red_pink) + facet_wrap(~set, ncol=2) + 
  xlab("X") + 
  ylab("Y") + 
  theme_pander(base_size = 17, base_family = "Fira Sans", fc = met_slate)
```

---
# Other Non-linear Relationships

 - Binary dependent variable
 
 - Interactions (covered later in the course)

 - Polynomial regressors (not covered)

---
# Binary Dependent Variable

.font80[

- Previously, introductory courses spent significant time arguing that binary dependent outcomes invalidated regression. 

- The two main reasons were: 
  1. This is a highly non-linear relationship (draw plot)
  2. The errors in this context have a variance that is correlated with the Xs (heteroskedasticity). 

- The approach we follow here does not focus on spending much time addressing this concerns. Because
  - Even if its non-linear, the CEF property #2 says that regression will find the best linear approximation. The key is to choose regressors well (in this case a collection of dummies probably will work better than a single slope). 
  - We now use robust standard errors pretty much all the time. 
  
- Regression in this context takes the name Linear Probability Model (the other methods not covered here are Logit and Probit estimation). 
]

---
class: inverse, middle

# (Polynomials Terms in a Regression)
## (Not Covered, but leaving it here in case you are interested. Requires a little knowing the derivative of polynomials)
## (Will mark each of the non-covered slides with an  [NC])


---
# Quadratic (and other Polynomial) Relationships [NC]

```{R, quad plot, echo = F, cache = T, dev = "svg", fig.height = 5}
# Set seed
set.seed(1234)
# Sample size
n <- 1e3
# Generate data
quad_df <- tibble(
  x = runif(n, 0, 10),
  y = 12 + 16*x - 2.5*x^2 + rnorm(n, sd = 25)
)
# regression
quad_reg <- lm(y ~ x + I(x^2), data = quad_df) 
b1 <- quad_reg$coefficients[2]
b2 <- quad_reg$coefficients[3]
# Plot
ggplot(data = quad_df, aes(x = x, y = y)) +
geom_point(size = 3, color = "darkslategrey", alpha = 0.5) +
xlab("X") +
ylab("Y") +
theme_pander(base_size = 17, base_family = "Fira Sans", fc = met_slate)
```

---
# Quadratic Regression  [NC]
.font80[
**Regression Model**

$$Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + e_i$$



**Interpretation** 

Sign of $\beta_2$ indicates whether the relationship is convex (.mono[+]) or concave (.mono[-])

Sign of $\beta_1$?  🤷

Partial derivative of $Y$ with respect to $X$ is the .hi[marginal effect] of $X$ on $Y$:

$$\color{#e64173}{\dfrac{\partial Y}{\partial X} = \beta_1 + 2 \beta_2 X}$$

- Effect of $X$ depends on the level of $X$
]
---
# Quadratic Regression [NC]

```{r}
lm(y ~ x + I(x^2), data = quad_df) %>% tidy()
```

.pink[What is the marginal effect of] $\color{#e64173}{X}$ .pink[on] $\color{#e64173}{Y}$.pink[?]
--
<br>
$\widehat{\dfrac{\partial \text{Y}}{\partial \text{X}} } = \hat{\beta}_1 + 2\hat{\beta}_2 X = `r round(b1, 2)` + `r round(b2*2, 2)`X$

---
# Quadratic Regression  [NC]

```{r}
lm(y ~ x + I(x^2), data = quad_df) %>% tidy()
```

.pink[What is the marginal effect of] $\color{#e64173}{X}$ .pink[on] $\color{#e64173}{Y}$ .pink[when] $\color{#e64173}{X=0}$.pink[?]
--
<br>
$\widehat{\dfrac{\partial \text{Y}}{\partial \text{X}} }\Bigg|_{\small \text{X}=0} = \hat{\beta}_1 = `r round(b1, 2)`$

---
# Quadratic Regression  [NC]

```{r}
lm(y ~ x + I(x^2), data = quad_df) %>% tidy()
```

.pink[What is the marginal effect of] $\color{#e64173}{X}$ .pink[on] $\color{#e64173}{Y}$ .pink[when] $\color{#e64173}{X=2}$.pink[?]
--
<br>
$\widehat{\dfrac{\partial \text{Y}}{\partial \text{X}} }\Bigg|_{\small \text{X}=2} = \hat{\beta}_1 + 2\hat{\beta}_2 \cdot (2) = `r round(b1, 2)` `r round(b2*2*2, 2)` = `r round(b1 + b2*2*2, 2)`$

---
# Quadratic Regression  [NC]

```{r}
lm(y ~ x + I(x^2), data = quad_df) %>% tidy()
```

.pink[What is the marginal effect of] $\color{#e64173}{X}$ .pink[on] $\color{#e64173}{Y}$ .pink[when] $\color{#e64173}{X=7}$.pink[?]
--
<br>
$\widehat{\dfrac{\partial \text{Y}}{\partial \text{X}} }\Bigg|_{\small \text{X}=7} = \hat{\beta}_1 + 2\hat{\beta}_2 \cdot (7) = `r round(b1, 2)` `r round(b2*2*7, 2)` = `r round(b1 + b2*2*7, 2)`$

---
class: white-slide

```{R, include=F}
reg <- lm(y ~ x + I(x^2), data = quad_df)
margs <- cplot(reg, x = "x", dx = "x", 
               what = "effect", draw = FALSE)
```

.center[**Fitted Regression Line**]
```{R, echo = F, cache = T, dev = "svg", fig.height = 5}
quad_df %>%  
  ggplot(aes(x = x, y = y)) + 
  geom_point(color = met_slate, alpha = 0.5, size = 3) +
  stat_smooth(method = 'lm', formula = y ~ poly(x,2), se = F, linetype = 1, color = red_pink, size = 1) +
  xlab("X") + 
  ylab("Y") + 
  theme_pander(base_size = 17, base_family = "Fira Sans", fc = met_slate)
```

---
class: white-slide

.center[**Marginal Effect of X on Y**]
```{R, echo = F, cache = T, dev = "svg", fig.height = 5}
margs %>%  
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals), color = red_pink, size = 1) +
  geom_line(aes(y = upper), linetype = 2, color = red_pink, size = 1) +
  geom_line(aes(y = lower), linetype = 2, color = red_pink, size = 1) +
  geom_hline(yintercept = 0, linetype = 1, color = met_slate) +
  xlab("X") + 
  ylab("Marginal Effect") + 
  theme_pander(base_size = 17, base_family = "Fira Sans", fc = met_slate)
```

---
# Quadratic Regression  [NC]

.font80[
**Where does the regression** $\hat{Y_i} = \hat{\beta}_0 + \hat{\beta}_1 X_i + \hat{\beta}_2 X_i^2$ ***turn*?**

- In other words, where is the peak (valley) of the fitted relationship?



**Step 1:** Take the derivative and set equal to zero.

$$\widehat{\dfrac{\partial \text{Y}}{\partial \text{X}} } = \hat{\beta}_1 + 2\hat{\beta}_2 X = 0$$



**Step 2:** Solve for $X$.

$$X = -\dfrac{\hat{\beta}_1}{2\hat{\beta}_2}$$



**Example:** Peak of previous regression occurs at $X = `r round(-b1/(2*b2), 2)`$.

]
---
# Acknowledgments

.pull-left[
- [Kyle Raze's Undergraduate Econometrics 1](https://github.com/kyleraze/EC320_Econometrics)
- MM
- [Science Po Econometrics Course](https://raw.githack.com/ScPoEcon/ScPoEconometrics-Slides/master/chapter_slr/chapter_slr.html#71)
- [Nick Huntington-Klein's Explanation of logs](file:///Users/fhoces/Desktop/sandbox/econ140summer2022/16_all_things_reg.html#38)
]
.pull-right[

]


```{r gen_pdf, include = FALSE, cache = FALSE, eval = TRUE}
pagedown::chrome_print("17_all_things_reg2.html", output = "17_all_things_reg2.pdf")
```